{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Ensamble Methods\n",
    "Here I compare different ensemble methods: Random Forest, Gradient Boosting Decision Tree, and XGBoost.\n",
    "\n",
    "There are 2 datasets, split into train and test sets the same way: test_size=0.33, random_state=42.\n",
    "\n",
    "- Compare several parameters options by cross validating with GridSearch.\n",
    "- Compare mean error rates and the confusion matrices on test data.\n",
    "- Compare run times and overall model quality for these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gihani Dissanayake\\Anaconda2\\envs\\py35\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = pd.read_csv('X1.csv', header=None)\n",
    "y1 = pd.read_csv('y1.csv', header=None)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1,y1,test_size = .33, random_state=42)\n",
    "\n",
    "X2 = pd.read_csv('X2.csv', header=None)\n",
    "y2 = pd.read_csv('y2.csv', header=None)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2,test_size = .33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 30), (10000, 1), (5000, 20), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape, y1.shape, X2.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'n_estimators': 12, 'criterion': 'gini'}\n",
      "[[1394  287]\n",
      " [ 164 1455]]\n",
      "mean error rate = 0.136666666667\n",
      "train time = 11.945666968507046   predict time = 0.005732498899833516\n"
     ]
    }
   ],
   "source": [
    "#Random Forest 1\n",
    "RFparam_grid = {'n_estimators':[11,12,13],\n",
    "              'criterion':['gini','entropy'],\n",
    "              'max_depth':[8,9,10]}\n",
    "\n",
    "RF = RandomForestClassifier(random_state = 42)\n",
    "RFmodel = GridSearchCV(RF, param_grid = RFparam_grid, cv=3)\n",
    "start_time = timeit.default_timer()\n",
    "RFmodel.fit(X1_train, y1_train[0])\n",
    "traintime = timeit.default_timer() - start_time\n",
    "\n",
    "print (RFmodel.best_params_)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "RF_prediction_test = RFmodel.predict(X1_test)\n",
    "predicttime = timeit.default_timer() - start_time\n",
    "\n",
    "RFcf = confusion_matrix(y1_test, RF_prediction_test)\n",
    "print (RFcf)\n",
    "\n",
    "RF1_mean_error_rate = (1.0*RFcf[0,1] + RFcf[1,0])/(RFcf[0,0] + RFcf[0,1] + RFcf[1,0] + RFcf[1,1])\n",
    "print (\"mean error rate = \" + str(RF1_mean_error_rate))\n",
    "print (\"train time = \" + str(traintime) + \"   predict time = \" + str(predicttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.2, 'n_estimators': 12, 'max_depth': 7}\n",
      "[[1521  160]\n",
      " [ 117 1502]]\n",
      "mean error rate = 0.0839393939394\n",
      "train time = 45.75317073445945   predict time = 0.002380519165448902\n"
     ]
    }
   ],
   "source": [
    "#GBDT 1\n",
    "GBparam_grid = {'n_estimators':[10,11,12],\n",
    "                'learning_rate': [.05,.1,.2,.3],\n",
    "                  'max_depth':[6,7,8]}\n",
    "\n",
    "GB = GradientBoostingClassifier(random_state = 42)\n",
    "GBmodel = GridSearchCV(GB, param_grid = GBparam_grid, cv=3)\n",
    "start_time = timeit.default_timer()\n",
    "GBmodel.fit(X1_train, y1_train[0])\n",
    "traintime = timeit.default_timer() - start_time\n",
    "\n",
    "print (GBmodel.best_params_)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "GB_prediction_test = GBmodel.predict(X1_test)\n",
    "predicttime = timeit.default_timer() - start_time\n",
    "\n",
    "GBcf = confusion_matrix(y1_test, GB_prediction_test)\n",
    "print (GBcf)\n",
    "\n",
    "GB1_mean_error_rate = (1.0*GBcf[0,1] + GBcf[1,0])/(GBcf[0,0] + GBcf[0,1] + GBcf[1,0] + GBcf[1,1])\n",
    "print (\"mean error rate = \" + str(GB1_mean_error_rate))\n",
    "print (\"train time = \" + str(traintime) + \"   predict time = \" + str(predicttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.3, 'n_estimators': 12, 'max_depth': 7}\n",
      "[[1535  146]\n",
      " [ 113 1506]]\n",
      "mean error rate = 0.0784848484848\n",
      "train time = 14.094143873486559   predict time = 0.0038128505015606606\n"
     ]
    }
   ],
   "source": [
    "#XGBoost 1\n",
    "XGparam_grid = {'n_estimators':[10,11,12],\n",
    "                'learning_rate': [.05,.1,.2,.3],\n",
    "                  'max_depth':[6,7,8]}\n",
    "\n",
    "XG = XGBClassifier()\n",
    "XGmodel = GridSearchCV(XG, param_grid = XGparam_grid, cv=3)\n",
    "start_time = timeit.default_timer()\n",
    "XGmodel.fit(X1_train, y1_train[0])\n",
    "traintime = timeit.default_timer() - start_time\n",
    "\n",
    "print (XGmodel.best_params_)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "XG_prediction_test = XGmodel.predict(X1_test)\n",
    "predicttime = timeit.default_timer() - start_time\n",
    "\n",
    "XGcf = confusion_matrix(y1_test, XG_prediction_test)\n",
    "print (XGcf)\n",
    "\n",
    "XG1_mean_error_rate = (1.0*XGcf[0,1] + XGcf[1,0])/(XGcf[0,0] + XGcf[0,1] + XGcf[1,0] + XGcf[1,1])\n",
    "print (\"mean error rate = \" + str(XG1_mean_error_rate))\n",
    "print (\"train time = \" + str(traintime) + \"   predict time = \" + str(predicttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'n_estimators': 13, 'criterion': 'gini'}\n",
      "[[768  82]\n",
      " [ 56 744]]\n",
      "mean error rate = 0.0836363636364\n",
      "train time = 5.365731102535463   predict time = 0.003749026776333153\n"
     ]
    }
   ],
   "source": [
    "#Random Forest 2\n",
    "RFparam_grid = {'n_estimators':[11,12,13],\n",
    "              'criterion':['gini','entropy'],\n",
    "              'max_depth':[8,9,10]}\n",
    "\n",
    "RF = RandomForestClassifier(random_state = 42)\n",
    "RFmodel = GridSearchCV(RF, param_grid = RFparam_grid, cv=3)\n",
    "start_time = timeit.default_timer()\n",
    "RFmodel.fit(X2_train, y2_train[0])\n",
    "traintime = timeit.default_timer() - start_time\n",
    "\n",
    "print (RFmodel.best_params_)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "RF2_prediction_test = RFmodel.predict(X2_test)\n",
    "predicttime = timeit.default_timer() - start_time\n",
    "\n",
    "RF2cf = confusion_matrix(y2_test, RF2_prediction_test)\n",
    "print (RF2cf)\n",
    "\n",
    "RF2_mean_error_rate = (1.0*RF2cf[0,1] + RF2cf[1,0])/(RF2cf[0,0] + RF2cf[0,1] + RF2cf[1,0] + RF2cf[1,1])\n",
    "print (\"mean error rate = \" + str(RF2_mean_error_rate))\n",
    "print (\"train time = \" + str(traintime) + \"   predict time = \" + str(predicttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.3, 'n_estimators': 12, 'max_depth': 6}\n",
      "[[770  80]\n",
      " [ 61 739]]\n",
      "mean error rate = 0.0854545454545\n",
      "train time = 19.583851328097666   predict time = 0.0015497528858219312\n"
     ]
    }
   ],
   "source": [
    "#GBDT 2\n",
    "GBparam_grid = {'n_estimators':[10,11,12],\n",
    "                'learning_rate': [.05,.1,.2,.3],\n",
    "                  'max_depth':[6,7,8]}\n",
    "\n",
    "GB = GradientBoostingClassifier(random_state = 42)\n",
    "GBmodel = GridSearchCV(GB, param_grid = GBparam_grid, cv=3)\n",
    "start_time = timeit.default_timer()\n",
    "GBmodel.fit(X2_train, y2_train[0])\n",
    "traintime = timeit.default_timer() - start_time\n",
    "\n",
    "print (GBmodel.best_params_)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "GB2_prediction_test = GBmodel.predict(X2_test)\n",
    "predicttime = timeit.default_timer() - start_time\n",
    "\n",
    "GB2cf = confusion_matrix(y2_test, GB2_prediction_test)\n",
    "print (GB2cf)\n",
    "\n",
    "GB2_mean_error_rate = (1.0*GB2cf[0,1] + GB2cf[1,0])/(GB2cf[0,0] + GB2cf[0,1] + GB2cf[1,0] + GB2cf[1,1])\n",
    "print (\"mean error rate = \" + str(GB2_mean_error_rate))\n",
    "print (\"train time = \" + str(traintime) + \"   predict time = \" + str(predicttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.3, 'n_estimators': 12, 'max_depth': 8}\n",
      "[[771  79]\n",
      " [ 48 752]]\n",
      "mean error rate = 0.0769696969697\n",
      "train time = 8.08106106766867   predict time = 0.001996871579606818\n"
     ]
    }
   ],
   "source": [
    "#XGBoost 2\n",
    "XGparam_grid = {'n_estimators':[10,11,12],\n",
    "                'learning_rate': [.05,.1,.2,.3],\n",
    "                  'max_depth':[6,7,8]}\n",
    "\n",
    "XG = XGBClassifier()\n",
    "XGmodel = GridSearchCV(XG, param_grid = XGparam_grid, cv=3)\n",
    "start_time = timeit.default_timer()\n",
    "XGmodel.fit(X2_train, y2_train[0])\n",
    "traintime = timeit.default_timer() - start_time\n",
    "\n",
    "print (XGmodel.best_params_)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "XG2_prediction_test = XGmodel.predict(X2_test)\n",
    "predicttime = timeit.default_timer() - start_time\n",
    "\n",
    "XG2cf = confusion_matrix(y2_test, XG2_prediction_test)\n",
    "print (XG2cf)\n",
    "\n",
    "XG2_mean_error_rate = (1.0*XG2cf[0,1] + XG2cf[1,0])/(XG2cf[0,0] + XG2cf[0,1] + XG2cf[1,0] + XG2cf[1,1])\n",
    "print (\"mean error rate = \" + str(XG2_mean_error_rate))\n",
    "print (\"train time = \" + str(traintime) + \"   predict time = \" + str(predicttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6700, 30)\n",
      "(3300, 30)\n",
      "(3350, 20)\n",
      "(1650, 20)\n"
     ]
    }
   ],
   "source": [
    "print (X1_train.shape)\n",
    "print (X1_test.shape)\n",
    "print (X2_train.shape)\n",
    "print (X2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF1 mean error rate is 0.136666666667\n",
      "GB1 mean error rate is 0.0839393939394\n",
      "XG1 mean error rate is 0.0784848484848\n",
      "RF2 mean error rate is 0.0836363636364\n",
      "GB2 mean error rate is 0.0854545454545\n",
      "XG2 mean error rate is 0.0769696969697\n"
     ]
    }
   ],
   "source": [
    "print (\"RF1 mean error rate is \" + str(RF1_mean_error_rate))\n",
    "print (\"GB1 mean error rate is \" + str(GB1_mean_error_rate))\n",
    "print (\"XG1 mean error rate is \" + str(XG1_mean_error_rate))\n",
    "print (\"RF2 mean error rate is \" + str(RF2_mean_error_rate))\n",
    "print (\"GB2 mean error rate is \" + str(GB2_mean_error_rate))\n",
    "print (\"XG2 mean error rate is \" + str(XG2_mean_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set 1 is much larger than data set 2, so we expected to be able to achieve better results from set 1. However, after tuning our parameters using grid search CV, we were able to obtain the lowest mean error rate using XG boost on set 2. Additionally, XG boost was fast to fit the data, taking ~8-13 seconds each time. XG boost was designed to be a more efficient and flexible algorithm, so it is no surprise that it performed the best in our tests. If we had unlimited time and computing resources we could cross validate more extensively and tune additional parameters in order to further improve our model. However, given our constraints, we are pleased with the accuracy of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Classification with Support Vector Machines\n",
    "\n",
    "A comparison between a linear SVM and Gaussian radial basis kernel using the Pima Indian Women diabetes detection problem on the [dataset](http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes).\n",
    "\n",
    "Default parametrs were held except for the slack penatlty \"C\" which was decided using grid search cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gihani Dissanayake\\Anaconda2\\envs\\py35\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import grid_search\n",
    "data_train = pd.read_csv('diabetes_train-log.csv')\n",
    "data_test = pd.read_csv('diabetes_test-log.csv')\n",
    "cols = ['numpreg', 'plasmacon', 'bloodpress', 'skinfold', 'seruminsulin', 'BMI', 'pedigreefunction', 'age']\n",
    "xtrain = np.asmatrix(data_train[cols])\n",
    "ytrain = np.asarray(data_train['classvariable']).T\n",
    "xtest = np.asmatrix(data_test[cols])\n",
    "ytest = np.asarray(data_test['classvariable']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1000}\n",
      "[[218  27]\n",
      " [ 45  67]]\n",
      "mean error rate = 0.201680672269\n"
     ]
    }
   ],
   "source": [
    "#a\n",
    "Cs = [.01,.1,1,10,100,1000]\n",
    "SVCmodel = SVC(kernel=\"linear\",random_state=42)\n",
    "SVM = GridSearchCV(cv=10, estimator = SVCmodel, param_grid = {'C':Cs})\n",
    "SVM.fit(xtrain, ytrain)\n",
    "print (SVM.best_params_)\n",
    "SVM_test_pred = SVM.predict(xtest)\n",
    "SVMcf = confusion_matrix(ytest, SVM_test_pred)\n",
    "print (SVMcf)\n",
    "print (\"mean error rate = \" + str((1.0*SVMcf[0,1] + SVMcf[1,0])/(SVMcf[0,0] + SVMcf[0,1] + SVMcf[1,0] + SVMcf[1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "[[205  40]\n",
      " [ 41  71]]\n",
      "mean error rate = 0.226890756303\n"
     ]
    }
   ],
   "source": [
    "#b\n",
    "Cs = [.01,.1,1,10,100,1000]\n",
    "SVCrbfmodel = SVC(kernel=\"rbf\",random_state=42)\n",
    "SVMrbf = GridSearchCV(cv=10, estimator = SVCrbfmodel, param_grid = {'C':Cs})\n",
    "SVMrbf.fit(xtrain, ytrain)\n",
    "print (SVMrbf.best_params_)\n",
    "SVMrbf_test_pred = SVMrbf.predict(xtest)\n",
    "SVMrbf_cf = confusion_matrix(ytest, SVMrbf_test_pred)\n",
    "print (SVMrbf_cf)\n",
    "print (\"mean error rate = \" + str((1.0*SVMrbf_cf[0,1] + SVMrbf_cf[1,0])/\n",
    "                                  (SVMrbf_cf[0,0] + SVMrbf_cf[0,1] + SVMrbf_cf[1,0] + SVMrbf_cf[1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean error rate is lower in the linear SVM  when compared to the Gaussian radial basis kernal. This demonstrates that the linear SVM is more accruate at predicting diabetes in the dataset because there are fewer false positives and false negatives. We tested Cs on a logarithmic scale to effeciently compare vastly different slack penalties. Doing this demonstrated that the linear SVM is optimized by a much larger slack variable than RBF."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
